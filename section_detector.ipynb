{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1814/2010_1814.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1815/2010_1815.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2266/2011_2266.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1757/2010_1757.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1759/2010_1759.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2062/2011_2062.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2203/2011_2203.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1732/2010_1732.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1760/2010_1760.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1758/2010_1758.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2052/2011_2052.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2258/2011_2258.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1756/2010_1756.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2205/2011_2205.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1734/2010_1734.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2229/2011_2229.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1727/2010_1727.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2024/2011_2024.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2211/2011_2211.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2079/2011_2079.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1728/2010_1728.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2226/2011_2226.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2040/2011_2040.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2078/2011_2078.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2071/2011_2071.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2076/2011_2076.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1272/2009_1272.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1210/2009_1210.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1217/2009_1217.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1808/2010_1808.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1228/2009_1228.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1221/2009_1221.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1466/2010_1466.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1226/2009_1226.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1806/2010_1806.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1219/2009_1219.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1227/2009_1227.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1218/2009_1218.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1807/2010_1807.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1467/2010_1467.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1220/2009_1220.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1800/2010_1800.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1216/2009_1216.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1229/2009_1229.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1211/2009_1211.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1469/2010_1469.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1252/2009_1252.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1263/2009_1263.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1817/2010_1817.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1819/2010_1819.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1479/2010_1479.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1254/2009_1254.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1811/2010_1811.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1209/2009_1209.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1816/2010_1816.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2207/2011_2207.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1798/2010_1798.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1754/2010_1754.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1796/2010_1796.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2237/2011_2237.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1739/2010_1739.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2033/2011_2033.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2069/2011_2069.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2056/2011_2056.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1755/2010_1755.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2241/2011_2241.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2223/2011_2223.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2249/2011_2249.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1740/2010_1740.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2075/2011_2075.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2214/2011_2214.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2010_1725/2010_1725.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1214/2009_1214.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1213/2009_1213.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1225/2009_1225.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1222/2009_1222.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2012_2712/2012_2712.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1271/2009_1271.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2011_2303/2011_2303.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1223/2009_1223.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1224/2009_1224.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1212/2009_1212.pdf...\n",
      "Processing /Users/AliTarik/Documents/LastAttempt/2009_1215/2009_1215.pdf...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import fitz\n",
    "\n",
    "def pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    coordinates = (10, 75, 550, 750)\n",
    "    for page_number in range(doc.page_count): \n",
    "        page = doc.load_page(page_number)\n",
    "        text = page.get_textbox(coordinates) \n",
    "        if text: \n",
    "            all_text.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "def extract_sections(text):\n",
    "    # Extract from ASSESSMENT to APPENDICES\n",
    "    assessment_to_appendices_pattern = re.compile(r'INFORMATION AS PROVIDED IN THE CONSOLIDATED LIST(.*?APPENDICES)', re.DOTALL)\n",
    "    assessment_to_appendices_match = assessment_to_appendices_pattern.search(text)\n",
    "    if not assessment_to_appendices_match:\n",
    "        return None, None, None, None\n",
    "    assessment_to_appendices = assessment_to_appendices_match.group(1)\n",
    "\n",
    "    # Check for the existence of \"Scientific substantiation of the claimed effect\"\n",
    "    sci_substantiation_pattern = re.compile(r'(Scientific substantiation of the claimed effect.*?)(comments on the proposed wording|Conditions and possible restrictions of use|CONCLUSIONS)', re.DOTALL)\n",
    "    sci_substantiation_match = sci_substantiation_pattern.search(assessment_to_appendices)\n",
    "    if not sci_substantiation_match:\n",
    "        return None, None, None, None\n",
    "    scientific_subs = sci_substantiation_match.group(1)\n",
    "\n",
    "    # Check if \"Conditions and possible restrictions of use\" exists\n",
    "    conditions_restrictions_pattern = re.compile(r'(Conditions and possible restrictions of use.*?)CONCLUSIONS', re.DOTALL)\n",
    "    conditions_restrictions_match = conditions_restrictions_pattern.search(assessment_to_appendices)\n",
    "\n",
    "    if conditions_restrictions_match:\n",
    "        conditions_restrictions = conditions_restrictions_match.group(1)\n",
    "    else:\n",
    "        conditions_restrictions = \"None\"\n",
    "    conclusions_pattern = re.compile(r'(CONCLUSIONS.*?)DOCUMENTATION PROVIDED TO EFSA', re.DOTALL)\n",
    "    conclusions_match = conclusions_pattern.search(assessment_to_appendices)\n",
    "    if conclusions_match:\n",
    "        conclusions = conclusions_match.group(1)\n",
    "    else: return None, None, None, None\n",
    "\n",
    "    references_pattern = re.compile(r'(REFERENCES.*?)APPENDICES', re.DOTALL)\n",
    "    references_match = references_pattern.search(assessment_to_appendices)\n",
    "    references = references_match.group(1) if references_match else \"\"\n",
    "\n",
    "    return scientific_subs, conditions_restrictions, conclusions, references\n",
    "\n",
    "def save_to_json(data, output_directory, filename):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def process_document(pdf_path):\n",
    "    output_dir = os.path.dirname(pdf_path) \n",
    "    output_filename = os.path.splitext(os.path.basename(pdf_path))[0] + '.json'\n",
    "\n",
    "    text = pdf_text(pdf_path)\n",
    "    scientific_subs, conditions_restrictions, conclusions, references = extract_sections(text)\n",
    "\n",
    "    if scientific_subs is None:\n",
    "        print(\"Scientific substantiation section not found.\")\n",
    "        return\n",
    "\n",
    "    data = {\n",
    "        \"scientific_substantiation\": scientific_subs,\n",
    "        \"conditions_restrictions\": conditions_restrictions,\n",
    "        \"conclusions\": conclusions,\n",
    "        \"references\": references\n",
    "    }\n",
    "    save_to_json(data, output_dir, output_filename)\n",
    "\n",
    "def preprocess_all_pdfs(input_dir):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing {pdf_path}...\")\n",
    "                process_document(pdf_path)\n",
    "\n",
    "def main():\n",
    "    input_dir = 'RootDirectoryOfPatentData'\n",
    "    preprocess_all_pdfs(input_dir)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2010_1814.json...\n",
      "Processing 2010_1815.json...\n",
      "Processing 2011_2266.json...\n",
      "Processing 2010_1757.json...\n",
      "Processing 2010_1759.json...\n",
      "Processing 2011_2062.json...\n",
      "Processing 2010_1732.json...\n",
      "Processing 2010_1760.json...\n",
      "Processing 2010_1758.json...\n",
      "Processing 2011_2052.json...\n",
      "Processing 2011_2258.json...\n",
      "Processing 2010_1756.json...\n",
      "Processing 2011_2205.json...\n",
      "Processing 2010_1734.json...\n",
      "Processing 2011_2229.json...\n",
      "Processing 2010_1727.json...\n",
      "Processing 2011_2024.json...\n",
      "Processing 2011_2211.json...\n",
      "Processing 2011_2079.json...\n",
      "Processing 2010_1728.json...\n",
      "Processing 2011_2226.json...\n",
      "Processing 2011_2040.json...\n",
      "Processing 2011_2078.json...\n",
      "Processing 2011_2071.json...\n",
      "Processing 2011_2076.json...\n",
      "Processing 2009_1272.json...\n",
      "Processing 2009_1210.json...\n",
      "Processing 2009_1217.json...\n",
      "Processing 2010_1808.json...\n",
      "Processing 2009_1228.json...\n",
      "Processing 2009_1221.json...\n",
      "Processing 2010_1466.json...\n",
      "Processing 2009_1226.json...\n",
      "Processing 2010_1806.json...\n",
      "Processing 2009_1219.json...\n",
      "Processing 2009_1227.json...\n",
      "Processing 2009_1218.json...\n",
      "Processing 2010_1807.json...\n",
      "Processing 2010_1467.json...\n",
      "Processing 2009_1220.json...\n",
      "Processing 2010_1800.json...\n",
      "Processing 2009_1216.json...\n",
      "Processing 2009_1229.json...\n",
      "Processing 2009_1211.json...\n",
      "Processing 2010_1469.json...\n",
      "Processing 2009_1252.json...\n",
      "Processing 2009_1263.json...\n",
      "Processing 2010_1817.json...\n",
      "Processing 2010_1819.json...\n",
      "Processing 2010_1479.json...\n",
      "Processing 2009_1254.json...\n",
      "Processing 2010_1811.json...\n",
      "Processing 2009_1209.json...\n",
      "Processing 2010_1816.json...\n",
      "Processing 2011_2207.json...\n",
      "Processing 2010_1798.json...\n",
      "Processing 2010_1796.json...\n",
      "Processing 2011_2237.json...\n",
      "Processing 2010_1739.json...\n",
      "Processing 2011_2033.json...\n",
      "Processing 2011_2069.json...\n",
      "Processing 2011_2056.json...\n",
      "Processing 2010_1755.json...\n",
      "Processing 2011_2223.json...\n",
      "Processing 2011_2249.json...\n",
      "Processing 2010_1740.json...\n",
      "Processing 2011_2075.json...\n",
      "Processing 2011_2214.json...\n",
      "Processing 2010_1725.json...\n",
      "Processing 2009_1214.json...\n",
      "Processing 2009_1213.json...\n",
      "Processing 2009_1225.json...\n",
      "Processing 2009_1222.json...\n",
      "Processing 2009_1271.json...\n",
      "Processing 2011_2303.json...\n",
      "Processing 2009_1223.json...\n",
      "Processing 2009_1224.json...\n",
      "Processing 2009_1212.json...\n",
      "Processing 2009_1215.json...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = text.replace('\\n', '')\n",
    "\n",
    "    clean_text = text.replace('\\\\', '')\n",
    "    # return re.sub(r'\\\\[\\n\\']', '', text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def extract_sections_with_ids(text):\n",
    "    pattern = r'([^\\n]*\\(ID [^\\)]+\\))(.*?)(?=\\n[^\\n]*\\(ID |$)'\n",
    "    matches = re.findall(pattern, text, re.DOTALL)\n",
    "    if not matches:\n",
    "        return [[\"One Claim\" , text]]\n",
    "    return matches\n",
    "\n",
    "def create_dictionary(matches):\n",
    "    # Split the match string into lines\n",
    "    dictionary = {}\n",
    "    for title_id, content in matches:\n",
    "        try:\n",
    "            title_id = clean_text(title_id.strip())\n",
    "            content = clean_text(content.strip())\n",
    "        except:\n",
    "            title_id = clean_text(title_id[0])\n",
    "            content = clean_text(content[0])\n",
    "\n",
    "        dictionary[title_id] = title_id + \"  \" + content\n",
    "    \n",
    "    return dictionary\n",
    "def save_output_to_file(directory,key, scientific_text, conditions_text, conclusion_text):\n",
    "    claims_directory = os.path.join(directory, 'claims')\n",
    "    os.makedirs(claims_directory, exist_ok=True)\n",
    "    filename = os.path.join(claims_directory, f\"{key.replace(' ', '_').replace('/', '_').replace(':', '_')}.txt\")\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Scientific Substantiation:\\n{scientific_text}\\n\\n\")\n",
    "        file.write(f\"Conditions and Restrictions:\\n{conditions_text}\\n\\n\")\n",
    "        file.write(f\"Conclusions:\\n{conclusion_text}\\n\\n\")\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json') and not file_name.endswith('data.json'):\n",
    "            print(f\"Processing {file_name}...\")\n",
    "            json_file_path = os.path.join(folder_path, file_name)\n",
    "            data = load_text_from_json(json_file_path)\n",
    "  \n",
    "\n",
    "\n",
    "            output_scientific = create_dictionary(extract_sections_with_ids(data[\"scientific_substantiation\"]))\n",
    "            output_conclusion = create_dictionary(extract_sections_with_ids(data[\"conclusions\"]))\n",
    "            for key in output_scientific.keys():\n",
    "                scientific_text = output_scientific[key]\n",
    "                if len(output_conclusion) == 1:\n",
    "                    conclusion_text = output_conclusion\n",
    "                else:\n",
    "                    conclusion_text = output_conclusion.get(key, \"No conclusion data available\")\n",
    "                conditions_text = data['conditions_restrictions']\n",
    "                save_output_to_file(folder_path, key, scientific_text, conditions_text, conclusion_text)\n",
    "\n",
    "def load_text_from_json(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data  \n",
    "\n",
    "def main():\n",
    "    root_directory = 'RootDirectoryOfPatentData'\n",
    "    for subdir in next(os.walk(root_directory))[1]:\n",
    "        process_folder(os.path.join(root_directory, subdir))\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
