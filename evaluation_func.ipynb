{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1759/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2062/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2205/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1734/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2211/claims/combined_data.json\n",
      "***********No extracted data************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2226/claims/combined_data.json\n",
      "***********No extracted data************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2040/claims/combined_data.json\n",
      "***********No extracted data************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2071/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2076/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1466/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1219/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1220/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1220/claims/combined_data.json\n",
      "***********No extracted data************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1800/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1211/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1211/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1263/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1817/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2207/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1796/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2237/claims/combined_data.json\n",
      "***********No extracted data************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1739/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2033/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2069/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1755/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2223/claims/combined_data.json\n",
      "***********Food mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2249/claims/combined_data.json\n",
      "***********No extracted data************\n",
      "/Users/AliTarik/Documents/LastAttempt/2011_2075/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2010_1725/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1213/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1271/claims/combined_data.json\n",
      "***********Phenotype mismatch in file************\n",
      "/Users/AliTarik/Documents/LastAttempt/2009_1223/claims/combined_data.json\n",
      "{'Food': {'TP': 53, 'FP': 0, 'FN': 14}, 'Phenotype': {'TP': 185, 'FP': 0, 'FN': 12}, 'Target Population': {'TP': 185, 'FP': 0, 'FN': 0}, 'Citations': {'TP': 501, 'FP': 0, 'FN': 164}}\n",
      "73\n",
      "Food - Precision: 1.0000, Recall: 0.7910\n",
      "Phenotype - Precision: 1.0000, Recall: 0.9391\n",
      "Target Population - Precision: 1.0000, Recall: 1.0000\n",
      "Citations - Precision: 1.0000, Recall: 0.7534\n"
     ]
    }
   ],
   "source": [
    "def get_doi(citations_data, citation_key):\n",
    "    citation_info = citations_data.get(citation_key, {})\n",
    "    return citation_info.get('doi', 'DOI not found')\n",
    "\n",
    "def get_pmid(citations_data, citation_key):\n",
    "    citation_info = citations_data.get(citation_key, {})\n",
    "    return citation_info.get('pmid', 'PMID not found')\n",
    "def check_text(text):\n",
    "    doi_pattern = r'^10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+$'\n",
    "\n",
    "    if re.match(doi_pattern, text, re.I):\n",
    "        return \"DOI\"\n",
    "\n",
    "    elif text.startswith(\"PMID \"):\n",
    "        return \"PMID\"\n",
    "\n",
    "    else:\n",
    "        return \"None of the patterns matched\"\n",
    "def extract_pmid(text):\n",
    "    match = re.search(r'PMID (\\d+)', text)\n",
    "    if match:\n",
    "        return match.group(1)  # Return the captured group of digits\n",
    "    return 'PMID not found'\n",
    "    \n",
    "def evaluate_data(validation_data, extracted_data,reference_data, model, tokenizer,extracted_file):\n",
    "    results = {\n",
    "        'Food': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'Phenotype': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'Target Population': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'Citations': {'TP': 0, 'FP': 0, 'FN': 0}\n",
    "    }\n",
    "    if extracted_data:\n",
    "        if extracted_data[0]['Food'].lower() == validation_data[0]['Food'].lower():\n",
    "            results['Food']['TP'] += 1  # All entries have correct Food\n",
    "        else:\n",
    "            results['Food']['FN'] += 1 \n",
    "           \n",
    "            \n",
    "            # print(validation_data[-1]['Food'])\n",
    "\n",
    "        primary_threshold = 0.75\n",
    "        for val_item in validation_data:\n",
    "            best_similarity = 0\n",
    "            best_match = None\n",
    "\n",
    "            for ext_item in extracted_data:\n",
    "                phenotype_similarity = calculate_similarity(val_item['Phenotype'], ext_item['Phenotype'], model, tokenizer)\n",
    "                if phenotype_similarity > best_similarity:\n",
    "                    best_similarity = phenotype_similarity\n",
    "                    best_match = ext_item\n",
    "\n",
    "            if best_similarity > primary_threshold:\n",
    "                results['Phenotype']['TP'] += 1\n",
    "                # Proceed to compare other fields only if the best phenotype match is strong enough\n",
    "                target_pop_similarity = calculate_similarity(val_item.get('Target Population', ''), best_match.get('Target Population', ''), model, tokenizer)\n",
    "                if target_pop_similarity > primary_threshold:\n",
    "                    results['Target Population']['TP'] += 1\n",
    "                else:\n",
    "                    results['Target Population']['FP'] += 1\n",
    "\n",
    "                val_citations = val_item.get('Citations', '').split(',')\n",
    "                val_dois = val_item.get('DOI', '').split(',')\n",
    "                ext_citations = best_match.get('Citations', '').split(';')\n",
    "                ext_dois = [get_doi(reference_data, citation_key) for citation_key in ext_citations]\n",
    "                ext_pmids = [get_pmid(reference_data, citation_key) for citation_key in ext_citations]\n",
    "\n",
    "                for citation , doi in zip(val_citations, val_dois):\n",
    "                    match_found_cite = False\n",
    "                    if check_text(doi)== 'DOI':\n",
    "                        for doi_ext in ext_dois:\n",
    "                            if doi_ext:\n",
    "                                if doi.lower() == doi_ext.lower():\n",
    "                                    match_found_cite = True\n",
    "                                    results['Citations']['TP'] += 1\n",
    "                                    break\n",
    "                        if not match_found_cite:\n",
    "                            for ext_citation in ext_citations:\n",
    "                                citation_similarity = calculate_similarity(citation, ext_citation, model, tokenizer)\n",
    "                                if citation_similarity > 0.9:\n",
    "                                    match_found_cite = True\n",
    "                                    results['Citations']['TP'] += 1\n",
    "                                    break\n",
    "                                \n",
    "                    elif check_text(doi)== 'PMID':\n",
    "                        for pmid_ext in ext_pmids:\n",
    "                            if extract_pmid(doi) == pmid_ext:\n",
    "                                match_found_cite = True\n",
    "                                results['Citations']['TP'] += 1\n",
    "                                break\n",
    "                        if not match_found_cite:\n",
    "                            for ext_citation in ext_citations:\n",
    "                                citation_similarity = calculate_similarity(citation, ext_citation, model, tokenizer)\n",
    "                                if citation_similarity > 0.9:\n",
    "                                    match_found_cite = True\n",
    "                                    results['Citations']['TP'] += 1\n",
    "                                    break\n",
    "                    else:\n",
    "                        for ext_citation in ext_citations:\n",
    "                            citation_similarity = calculate_similarity(citation, ext_citation, model, tokenizer)\n",
    "                            if citation_similarity > 0.9:\n",
    "                                match_found_cite = True\n",
    "                                results['Citations']['TP'] += 1\n",
    "                                break\n",
    "                    if not match_found_cite:\n",
    "                        results['Citations']['FN'] += 1\n",
    "\n",
    "            else:\n",
    "                results['Phenotype']['FN'] += 1\n",
    "    return results\n",
    "\n",
    "\n",
    "def calculate_similarity(text1, text2, model, tokenizer):\n",
    "    encoded_input1 = tokenizer(text1, return_tensors='pt', padding=True, truncation=True)\n",
    "    encoded_input2 = tokenizer(text2, return_tensors='pt', padding=True, truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model_output1 = model(**encoded_input1)\n",
    "        model_output2 = model(**encoded_input2)\n",
    "    \n",
    "    embeddings1 = model_output1.last_hidden_state.mean(dim=1)\n",
    "    embeddings2 = model_output2.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_sim = 1 - cosine(embeddings1.squeeze().numpy(), embeddings2.squeeze().numpy())\n",
    "    return cosine_sim\n",
    "\n",
    "def calculate_precision_recall(true_positives, false_positives, false_negatives):\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    return precision, recall\n",
    "\n",
    "def evaluate_directory(input_directory, model, tokenizer):\n",
    "    aggregate_results = {\n",
    "        'Food': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'Phenotype': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'Target Population': {'TP': 0, 'FP': 0, 'FN': 0},\n",
    "        'Citations': {'TP': 0, 'FP': 0, 'FN': 0}\n",
    "    }\n",
    "    \n",
    "    for root, dirs, files in os.walk(input_directory):\n",
    "        validation_file = os.path.join(root, 'data.json')\n",
    "        extracted_file = os.path.join(root, 'claims', 'combined_data.json')\n",
    "        reference_file = os.path.join(root,'claims', 'citation_references.json')\n",
    "        \n",
    "        if os.path.exists(validation_file) and os.path.exists(extracted_file):\n",
    "            count += 1\n",
    "            validation_data = load_data(validation_file)\n",
    "            extracted_data = load_data(extracted_file)\n",
    "            reference_data = load_data(reference_file)\n",
    "            results = evaluate_data(validation_data, extracted_data, reference_data, model, tokenizer,extracted_file)\n",
    "            \n",
    "            for category in aggregate_results:\n",
    "                aggregate_results[category]['TP'] += results[category]['TP']\n",
    "                aggregate_results[category]['FP'] += results[category]['FP']\n",
    "                aggregate_results[category]['FN'] += results[category]['FN']\n",
    "\n",
    "    # Calculate overall precision and recall for each category\n",
    "    final_results = {category: calculate_precision_recall(aggregate_results[category]['TP'],\n",
    "                                                          aggregate_results[category]['FP'],\n",
    "                                                          aggregate_results[category]['FN'])\n",
    "                     for category in aggregate_results}\n",
    "\n",
    "    return final_results\n",
    "def load_data(file_path):\n",
    "    import json\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "def get_model_and_tokenizer(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return model, tokenizer\n",
    "def main():\n",
    "    input_directory = 'RootDirectory'\n",
    "    model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "    model, tokenizer = get_model_and_tokenizer(model_name)\n",
    "    results = evaluate_directory(input_directory, model, tokenizer)\n",
    "    for category, metrics in results.items():\n",
    "        precision, recall = metrics\n",
    "        print(f'{category} - Precision: {precision:.4f}, Recall: {recall:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
