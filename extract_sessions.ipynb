{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from openai==0.28) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from openai==0.28) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.31.0\n",
      "    Uninstalling openai-1.31.0:\n",
      "      Successfully uninstalled openai-1.31.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llm 0.13.1 requires openai>=1.0, but you have openai 0.28.0 which is incompatible.\n",
      "ontogpt 0.3.12 requires openai<2.0.0,>=1.10.0, but you have openai 0.28.0 which is incompatible.\n",
      "ontogpt 0.3.12 requires ruamel.yaml>=0.17.31, but you have ruamel-yaml 0.16.13 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import textract\n",
    "import os\n",
    "import openai\n",
    "import fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    coordinates = (10, 75, 550, 750)\n",
    "    for page_number in range(doc.page_count): \n",
    "        page = doc.load_page(page_number)\n",
    "        text = page.get_textbox(coordinates) \n",
    "        if text: \n",
    "            all_text.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)  # Join all text pieces with a newline\n",
    "\n",
    "def extract_sections(text):\n",
    "    # Extract from ASSESSMENT to APPENDICES\n",
    "    assessment_to_appendices_pattern = re.compile(r'INFORMATION AS PROVIDED IN THE CONSOLIDATED LIST(.*?APPENDICES)', re.DOTALL)\n",
    "    assessment_to_appendices_match = assessment_to_appendices_pattern.search(text)\n",
    "    if not assessment_to_appendices_match:\n",
    "        return None, None, None, None\n",
    "    assessment_to_appendices = assessment_to_appendices_match.group(1)\n",
    "\n",
    "    # Check for the existence of \"Scientific substantiation of the claimed effect\"\n",
    "    sci_substantiation_pattern = re.compile(r'Scientific substantiation of the claimed effect(.*?)(Panelâ€™s comments on the proposed wording|Conditions and possible restrictions of use|CONCLUSIONS)', re.DOTALL)\n",
    "    sci_substantiation_match = sci_substantiation_pattern.search(assessment_to_appendices)\n",
    "    if not sci_substantiation_match:\n",
    "        return None, None, None, None\n",
    "    scientific_subs = sci_substantiation_match.group(1)\n",
    "\n",
    "    # Check if \"Conditions and possible restrictions of use\" exists\n",
    "    conditions_restrictions_pattern = re.compile(r'Conditions and possible restrictions of use(.*?)CONCLUSIONS', re.DOTALL)\n",
    "    conditions_restrictions_match = conditions_restrictions_pattern.search(assessment_to_appendices)\n",
    "\n",
    "    if conditions_restrictions_match:\n",
    "        conditions_restrictions = conditions_restrictions_match.group(1)\n",
    "    else:\n",
    "        conditions_restrictions = \"None\"\n",
    "    conclusions_pattern = re.compile(r'CONCLUSIONS(.*?)DOCUMENTATION PROVIDED TO EFSA', re.DOTALL)\n",
    "    conclusions_match = conclusions_pattern.search(assessment_to_appendices)\n",
    "    if conclusions_match:\n",
    "        conclusions = conclusions_match.group(1)\n",
    "    else: return None, None, None, None\n",
    "\n",
    "    references_pattern = re.compile(r'REFERENCES(.*?)APPENDICES', re.DOTALL)\n",
    "    references_match = references_pattern.search(assessment_to_appendices)\n",
    "    references = references_match.group(1) if references_match else \"\"\n",
    "\n",
    "    return scientific_subs, conditions_restrictions, conclusions, references\n",
    "\n",
    "def save_to_json(data, output_directory, filename):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def process_document(pdf_path):\n",
    "    output_dir = os.path.dirname(pdf_path)  # Use PDF's own directory to save the output\n",
    "    output_filename = os.path.splitext(os.path.basename(pdf_path))[0] + '.json'\n",
    "\n",
    "    text = pdf_text(pdf_path)\n",
    "    scientific_subs, conditions_restrictions, conclusions, references = extract_sections(text)\n",
    "\n",
    "    if scientific_subs is None:\n",
    "        print(\"Scientific substantiation section not found.\")\n",
    "        return\n",
    "\n",
    "    data = {\n",
    "        \"scientific_substantiation\": scientific_subs,\n",
    "        \"conditions_restrictions\": conditions_restrictions,\n",
    "        \"conclusions\": conclusions,\n",
    "        \"references\": references\n",
    "    }\n",
    "    save_to_json(data, output_dir, output_filename)\n",
    "\n",
    "def preprocess_all_pdfs(input_dir):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing {pdf_path}...\")\n",
    "                process_document(pdf_path)\n",
    "\n",
    "def main():\n",
    "    input_dir = '/EFSA_DOCUMENTATION'\n",
    "    preprocess_all_pdfs(input_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1759/2010_1759.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1760/2010_1760.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1758/2010_1758.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1798/2010_1798.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1796/2010_1796.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1764/2010_1764.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1763/2010_1763.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DOCUMENTATION/2010_1797/2010_1797.pdf...\n",
      "Scientific substantiation section not found.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_text_from_json(json_file):\n",
    "    \"\"\"Load text data from a JSON file.\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data  # Adjust 'text' depending on your JSON structure.\n",
    "\n",
    "def query_chatgpt(text, api_key):\n",
    "    \"\"\"Send a text query to ChatGPT and return the response.\"\"\"\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-4o\",  # Ensure this is the correct model for chat completions\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": text}\n",
    "      ],\n",
    "      max_tokens=4096 \n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip() \n",
    "\n",
    "def run_for_scientific(text):\n",
    "      # Change to your actual JSON file path\n",
    "    api_key = 'set your api key'  # Set your OpenAI API key\n",
    "\n",
    "    \n",
    "    prompt = f\"\"\"Extract the following without changing the words as I instruct from the given text:\\n\n",
    "                \n",
    "                    Title: The piece of text given as a subtitle in the text. Example titles include \"Energy-yielding metabolism (ID 114, 117)\" and \" Maintenance of skin and mucous membranes (ID 115)\". If there is no title matches the description, then set the title as \"Conclusion\". Title either must be in the format of \"Title (ID 123, ID 456)\" which exists in the text or if there is no titles \"Conclusion\".\\n\n",
    "                    Context: Piece of text provided under the title, offering detailed information related to the title. Citations should be kept. It must end by similar example: \"The Panel concludes that a cause and effect relationship has been established between the dietary intake of biotin and normal macronutrient metabolism. However, the evidence provided does not establish that inadequate intake of biotin leading to impaired macronutrient metabolism occurs in the general EU population\" or like \"The Panel concludes that a cause and effect relationship has been established between the consumption of live yoghurt cultures in yoghurt and improved digestion of lactose in yoghurt in individuals with lactose maldigestion.\". \n",
    "                                In case there is no detected title, return the whole text privided for you as input from the first word to last. take the text until the end in that case without eny restriction.\n",
    "\n",
    "\n",
    "                Do not write antyhing else other than the text you are asked to extract.\n",
    "                do not put anything like ``` or \"json\"\n",
    "                Return a dictionary format as follows in the example and nothing else: \n",
    "                {{\n",
    "                    \"title you find\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anemia. The Panel concludes that a cause and effect relationship has been established between the dietary intake of iron and normal oxygen transport to tissues.\"\n",
    "                    \"title you find\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anaemia. The Panel concludes that a cause and effect relationship has been established between the intake of iron and normal formation of red blood cells and haemoglobin.\"\n",
    "                    and so on \n",
    "                    }}\n",
    "            \\n\\n + {text}\"\"\"\n",
    "    \n",
    "    extracted_text = query_chatgpt(prompt, api_key)\n",
    "    return extracted_text   \n",
    "\n",
    "def run_for_conclusion(text):\n",
    "      # Change to your actual JSON file path\n",
    "    api_key = 'set your api key'  # Set your OpenAI API key\n",
    "\n",
    "    \n",
    "    prompt = f\"\"\"Extract the following without changing the words as I instruct from the given text:\\n\n",
    "                 \n",
    "                    Title: The piece of text given as a subtitle in the text. Example titles include \"Energy-yielding metabolism (ID 114, 117)\" and \" Maintenance of skin and mucous membranes (ID 115)\" . If there is no title return \"Conclusion\" as title\\n\n",
    "                    Context: Piece of text provided under the title, offering detailed information related to the title. Citations should be kept.In case there is no detected title, return the full text given to you as context. it ususally ends by the information about the target population. in case there is no detected title, return the whole text privided for you as input from the first word to last. take the text until the end in that case.\n",
    "                \n",
    "                Do not write antyhing else other than the text you are asked to extract.\n",
    "                do not put anything like ``` or \"json\"\n",
    "                Return a dictionary format as follows in the example and nothing else: \n",
    "                {{\n",
    "                    \"Oxygen transport (ID 250, ID 254, ID 256)\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anemia. The Panel concludes that a cause and effect relationship has been established between the dietary intake of iron and normal oxygen transport to tissues.\"\n",
    "                    \"Formation of red blood cells and haemoglobin (ID 249, ID 1589)\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anaemia. The Panel concludes that a cause and effect relationship has been established between the intake of iron and normal formation of red blood cells and haemoglobin.\"\n",
    "                    and so on \n",
    "                    }}\n",
    "            \\n\\n + {text}\"\"\"\n",
    "    \n",
    "    extracted_text = query_chatgpt(prompt, api_key)\n",
    "    return extracted_text\n",
    "\n",
    "def save_output_to_file(directory,key, scientific_text, conditions_text, conclusion_text, references_text):\n",
    "    claims_directory = os.path.join(directory, 'claims')\n",
    "    os.makedirs(claims_directory, exist_ok=True)  # Create the subfolder if it does not exist\n",
    "    filename = os.path.join(claims_directory, f\"{key.replace(' ', '_').replace('/', '_').replace(':', '_')}.txt\")\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Scientific Substantiation:\\n{scientific_text}\\n\\n\")\n",
    "        file.write(f\"Conditions and Restrictions:\\n{conditions_text}\\n\\n\")\n",
    "        file.write(f\"Conclusions:\\n{conclusion_text}\\n\\n\")\n",
    "        file.write(f\"References:\\n{references_text}\")\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"Processes all JSON files in a given folder.\"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            json_file_path = os.path.join(folder_path, file_name)\n",
    "            data = load_text_from_json(json_file_path)\n",
    "            \n",
    "            output_scientific = run_for_scientific(data[\"scientific_substantiation\"])\n",
    "            output_conclusion = run_for_conclusion(data[\"conclusions\"])\n",
    "            # print(output_scientific)\n",
    "            # print(output_conclusion)\n",
    "            try:\n",
    "                dictionary_scientific = json.loads(output_scientific)\n",
    "            except:\n",
    "                print(\"Error in scientific substantiation\")\n",
    "                print(output_scientific)\n",
    "            try:\n",
    "                dictionary_conclusion = json.loads(output_conclusion)\n",
    "            except:\n",
    "                print(\"Error in conclusion\")\n",
    "                print(output_conclusion)\n",
    "            # dictionary_conclusion = json.loads(output_conclusion)\n",
    "\n",
    "            if \"conclusion\" in dictionary_conclusion.keys():\n",
    "                first_key_in_scientific = next(iter(dictionary_scientific))\n",
    "                scientific_text = dictionary_scientific[first_key_in_scientific]\n",
    "                conclusion_text = dictionary_conclusion.get(\"conclusion\", \"No conclusion data available\")\n",
    "                conditions_text = data['conditions_restrictions']\n",
    "                references_text = data['references']\n",
    "\n",
    "                save_output_to_file(folder_path, \"conclusion\", scientific_text, conditions_text, conclusion_text, references_text)\n",
    "            else:\n",
    "                for key in dictionary_scientific.keys():\n",
    "                    scientific_text = dictionary_scientific[key]\n",
    "                    conclusion_text = dictionary_conclusion.get(key, \"No conclusion data available\")\n",
    "                    conditions_text = data['conditions_restrictions']\n",
    "                    references_text = data['references']\n",
    "                    save_output_to_file(folder_path, key, scientific_text, conditions_text, conclusion_text, references_text)\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_directory = '/EFSA_DOCUMENTATION'\n",
    "    for subdir in next(os.walk(root_directory))[1]:\n",
    "        process_folder(os.path.join(root_directory, subdir))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
