{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai==0.28\n",
      "  Using cached openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/site-packages (from openai==0.28) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/site-packages (from openai==0.28) (4.66.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/site-packages (from openai==0.28) (3.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->openai==0.28) (4.0.3)\n",
      "Using cached openai-0.28.0-py3-none-any.whl (76 kB)\n",
      "\u001b[33mDEPRECATION: textract 1.6.5 has a non-standard dependency specifier extract-msg<=0.29.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of textract or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.34.0\n",
      "    Uninstalling openai-1.34.0:\n",
      "      Successfully uninstalled openai-1.34.0\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "llm 0.13.1 requires openai>=1.0, but you have openai 0.28.0 which is incompatible.\n",
      "ontogpt 0.3.12 requires openai<2.0.0,>=1.10.0, but you have openai 0.28.0 which is incompatible.\n",
      "ontogpt 0.3.12 requires ruamel.yaml>=0.17.31, but you have ruamel-yaml 0.16.13 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import textract\n",
    "import os\n",
    "import openai\n",
    "import fitz\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "    coordinates = (10, 75, 550, 750)\n",
    "    for page_number in range(doc.page_count): \n",
    "        page = doc.load_page(page_number)\n",
    "        text = page.get_textbox(coordinates) \n",
    "        if text: \n",
    "            all_text.append(text)\n",
    "    \n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)  # Join all text pieces with a newline\n",
    "\n",
    "def extract_sections(text):\n",
    "    # Extract from ASSESSMENT to APPENDICES\n",
    "    assessment_to_appendices_pattern = re.compile(r'INFORMATION AS PROVIDED IN THE CONSOLIDATED LIST(.*?APPENDICES)', re.DOTALL)\n",
    "    assessment_to_appendices_match = assessment_to_appendices_pattern.search(text)\n",
    "    if not assessment_to_appendices_match:\n",
    "        return None, None, None, None\n",
    "    assessment_to_appendices = assessment_to_appendices_match.group(1)\n",
    "\n",
    "    # Check for the existence of \"Scientific substantiation of the claimed effect\"\n",
    "    sci_substantiation_pattern = re.compile(r'(Scientific substantiation of the claimed effect.*?)(comments on the proposed wording|Conditions and possible restrictions of use|CONCLUSIONS)', re.DOTALL)\n",
    "    sci_substantiation_match = sci_substantiation_pattern.search(assessment_to_appendices)\n",
    "    if not sci_substantiation_match:\n",
    "        return None, None, None, None\n",
    "    scientific_subs = sci_substantiation_match.group(1)\n",
    "\n",
    "    # Check if \"Conditions and possible restrictions of use\" exists\n",
    "    conditions_restrictions_pattern = re.compile(r'(Conditions and possible restrictions of use.*?)CONCLUSIONS', re.DOTALL)\n",
    "    conditions_restrictions_match = conditions_restrictions_pattern.search(assessment_to_appendices)\n",
    "\n",
    "    if conditions_restrictions_match:\n",
    "        conditions_restrictions = conditions_restrictions_match.group(1)\n",
    "    else:\n",
    "        conditions_restrictions = \"None\"\n",
    "    conclusions_pattern = re.compile(r'(CONCLUSIONS.*?)DOCUMENTATION PROVIDED TO EFSA', re.DOTALL)\n",
    "    conclusions_match = conclusions_pattern.search(assessment_to_appendices)\n",
    "    if conclusions_match:\n",
    "        conclusions = conclusions_match.group(1)\n",
    "    else: return None, None, None, None\n",
    "\n",
    "    references_pattern = re.compile(r'(REFERENCES.*?)APPENDICES', re.DOTALL)\n",
    "    references_match = references_pattern.search(assessment_to_appendices)\n",
    "    references = references_match.group(1) if references_match else \"\"\n",
    "\n",
    "    return scientific_subs, conditions_restrictions, conclusions, references\n",
    "\n",
    "def save_to_json(data, output_directory, filename):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    file_path = os.path.join(output_directory, filename)\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "def process_document(pdf_path):\n",
    "    output_dir = os.path.dirname(pdf_path)  # Use PDF's own directory to save the output\n",
    "    output_filename = os.path.splitext(os.path.basename(pdf_path))[0] + '.json'\n",
    "\n",
    "    text = pdf_text(pdf_path)\n",
    "    scientific_subs, conditions_restrictions, conclusions, references = extract_sections(text)\n",
    "\n",
    "    if scientific_subs is None:\n",
    "        print(\"Scientific substantiation section not found.\")\n",
    "        return\n",
    "\n",
    "    data = {\n",
    "        \"scientific_substantiation\": scientific_subs,\n",
    "        \"conditions_restrictions\": conditions_restrictions,\n",
    "        \"conclusions\": conclusions,\n",
    "        \"references\": references\n",
    "    }\n",
    "    save_to_json(data, output_dir, output_filename)\n",
    "\n",
    "def preprocess_all_pdfs(input_dir):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(root, file)\n",
    "                print(f\"Processing {pdf_path}...\")\n",
    "                process_document(pdf_path)\n",
    "\n",
    "def main():\n",
    "    input_dir = '/Users/AliTarik/Documents/EFSA_DCUMENTATION'\n",
    "    preprocess_all_pdfs(input_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1813/2010_1813.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1814/2010_1814.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1258/2009_1258.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1815/2010_1815.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1486/2010_1486.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2266/2011_2266.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1757/2010_1757.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1759/2010_1759.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2062/2011_2062.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2203/2011_2203.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1732/2010_1732.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1760/2010_1760.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1758/2010_1758.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2052/2011_2052.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2258/2011_2258.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1756/2010_1756.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2205/2011_2205.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1734/2010_1734.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2229/2011_2229.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1727/2010_1727.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2024/2011_2024.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2211/2011_2211.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1745/2010_1745.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2079/2011_2079.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1728/2010_1728.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2226/2011_2226.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2040/2011_2040.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2078/2011_2078.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2071/2011_2071.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2076/2011_2076.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2049/2011_2049.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1272/2009_1272.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1210/2009_1210.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1468/2010_1468.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1217/2009_1217.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1808/2010_1808.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1228/2009_1228.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1221/2009_1221.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1466/2010_1466.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1226/2009_1226.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1806/2010_1806.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1219/2009_1219.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1227/2009_1227.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1218/2009_1218.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1807/2010_1807.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1467/2010_1467.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1220/2009_1220.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1800/2010_1800.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1216/2009_1216.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1229/2009_1229.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1211/2009_1211.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1469/2010_1469.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1252/2009_1252.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1263/2009_1263.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1817/2010_1817.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1819/2010_1819.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1479/2010_1479.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1254/2009_1254.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1811/2010_1811.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1209/2009_1209.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1816/2010_1816.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1236/2009_1236.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2207/2011_2207.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1798/2010_1798.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1754/2010_1754.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1796/2010_1796.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2237/2011_2237.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1739/2010_1739.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2033/2011_2033.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1764/2010_1764.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1763/2010_1763.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1797/2010_1797.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2069/2011_2069.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2056/2011_2056.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1755/2010_1755.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2241/2011_2241.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2074/2011_2074.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2223/2011_2223.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1747/2010_1747.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2249/2011_2249.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1740/2010_1740.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2043/2011_2043.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2075/2011_2075.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2072/2011_2072.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2214/2011_2214.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1725/2010_1725.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1214/2009_1214.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1213/2009_1213.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1225/2009_1225.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1222/2009_1222.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2012_2712/2012_2712.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1271/2009_1271.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2303/2011_2303.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2011_2304/2011_2304.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2010_1464/2010_1464.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1223/2009_1223.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2012_2713/2012_2713.pdf...\n",
      "Scientific substantiation section not found.\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1224/2009_1224.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1212/2009_1212.pdf...\n",
      "Processing /Users/AliTarik/Documents/EFSA_DCUMENTATION/2009_1215/2009_1215.pdf...\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2011_2266.json...\n",
      "Processing 2011_2258.json...\n",
      "Processing 2011_2303.json...\n",
      "Processing 2011_2304.json...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def load_text_from_json(json_file):\n",
    "    \"\"\"Load text data from a JSON file.\"\"\"\n",
    "    with open(json_file, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data  # Adjust 'text' depending on your JSON structure.\n",
    "\n",
    "def query_chatgpt(text, api_key):\n",
    "    \"\"\"Send a text query to ChatGPT and return the response.\"\"\"\n",
    "    openai.api_key = api_key\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "      model=\"gpt-4o\",  # Ensure this is the correct model for chat completions\n",
    "      messages=[\n",
    "          {\"role\": \"user\", \"content\": text}\n",
    "      ],\n",
    "      max_tokens=4096 \n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip() \n",
    "\n",
    "def run_for_scientific(text):\n",
    "      # Change to your actual JSON file path\n",
    "    api_key = 'openai_API_key_here'  # Set your OpenAI API key\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    \n",
    "    prompt = f\"\"\"Extract the following without changing the words as I instruct from the given text:\\n\n",
    "                \n",
    "                    Title: The piece of text given as a subtitle in the text. Example titles include \"Energy-yielding metabolism (ID 114, 117)\" and \" Maintenance of skin and mucous membranes (ID 115)\". If you find the titles as in the example: Energy-yielding metabolism (ID 114, 117), take them as title\\n\n",
    "                    Context: Piece of text provided under the title, offering detailed information related to the title. Citations should be kept. It must end by similar example: \"The Panel concludes that a cause and effect relationship has been established between the dietary intake of biotin and normal macronutrient metabolism. However, the evidence provided does not establish that inadequate intake of biotin leading to impaired macronutrient metabolism occurs in the general EU population\" or like \"The Panel concludes that a cause and effect relationship has been established between the consumption of live yoghurt cultures in yoghurt and improved digestion of lactose in yoghurt in individuals with lactose maldigestion.\". \n",
    "                        If \"Scientific substantiation of the claimed effect\" has ID at the end, then take the whole text until the end as the context.\n",
    "\n",
    "\n",
    "                Do not write antyhing else other than the text you are asked to extract.\n",
    "                do not put anything like ``` or \"json\"\n",
    "                Do not put anything without title because it will be out of the json format.\n",
    "                it there is anything between double quataion marks in the content, make it single quotation marks.\n",
    "                Return a dictionary format as follows in the example and nothing else: \n",
    "                {{\n",
    "                    \"title you find\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anemia. The Panel concludes that a cause and effect relationship has been established between the dietary intake of iron and normal oxygen transport to tissues.\",\n",
    "                    \"title you find\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anaemia. The Panel concludes that a cause and effect relationship has been established between the intake of iron and normal formation of red blood cells and haemoglobin.\" \n",
    "                    }}\n",
    "            \\n\\n + {text}\"\"\"\n",
    "    \n",
    "    extracted_text = query_chatgpt(prompt, api_key)\n",
    "    # print(f\" Scientific Substantiation is: {extracted_text}\")\n",
    "    return extracted_text   \n",
    "\n",
    "def run_for_conclusion(text):\n",
    "      # Change to your actual JSON file path\n",
    "    api_key = 'openai_API_key_here'  # Set your OpenAI API key  # Set your OpenAI API key\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    text = text.replace(\"\\\\\", \" \")\n",
    "    \n",
    "    prompt = f\"\"\"Extract the following without changing the words as I instruct from the given text:\\n\n",
    "                 \n",
    "                    Title: The piece of text given as a subtitle in the text. Example titles \"Energy-yielding metabolism (ID 114, 117)\" and \" Maintenance of skin and mucous membranes (ID 115)\" . \\n\n",
    "                    Context: Piece of text provided under the title, offering detailed information related to the title. Citations should be kept.In case there is no detected title, return the full text given to you as context. it ususally ends by the information about the target population. \n",
    "                \n",
    "                Do not write antyhing else other than the text you are asked to extract.\n",
    "                do not put anything like ``` or \"json\"\n",
    "                Return a dictionary format ins json structure as follows in the example and nothing else: \n",
    "                {{\n",
    "                    \"Oxygen transport (ID 250, ID 254, ID 256)\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anemia. The Panel concludes that a cause and effect relationship has been established between the dietary intake of iron and normal oxygen transport to tissues.\",\n",
    "                    \"Formation of red blood cells and haemoglobin (ID 249, ID 1589)\": \"In humans, iron is mainly found in porphyrins. In haemproteins (haemoglobin and myoglobin) iron is found in its ferrous state (Fe2+) which allows it to bind oxygen reversibly. Haemoglobin transports oxygen in the erythrocytes to the tissues (Hunt, 2005). It is well established that inadequate dietary iron intake in humans leads to hypochromic and microcytic anaemia. A cause and effect relationship has not been established between the consumption of EPA and DHA and long-term maintenance of normal blood glucose concentrations.\" \n",
    "                    }}\n",
    "            \\n\\n + {text}\"\"\"\n",
    "    \n",
    "    extracted_text = query_chatgpt(prompt, api_key)\n",
    "    return extracted_text\n",
    "\n",
    "def save_output_to_file(directory,key, scientific_text, conditions_text, conclusion_text, references_text):\n",
    "    claims_directory = os.path.join(directory, 'claims')\n",
    "    os.makedirs(claims_directory, exist_ok=True)  # Create the subfolder if it does not exist\n",
    "    filename = os.path.join(claims_directory, f\"{key.replace(' ', '_').replace('/', '_').replace(':', '_')}.txt\")\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Scientific Substantiation:\\n{scientific_text}\\n\\n\")\n",
    "        file.write(f\"Conditions and Restrictions:\\n{conditions_text}\\n\\n\")\n",
    "        file.write(f\"Conclusions:\\n{conclusion_text}\\n\\n\")\n",
    "\n",
    "def process_folder(folder_path):\n",
    "    \"\"\"Processes all JSON files in a given folder.\"\"\"\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.json') and not file_name.endswith('data.json'):\n",
    "            print(f\"Processing {file_name}...\")\n",
    "            json_file_path = os.path.join(folder_path, file_name)\n",
    "            data = load_text_from_json(json_file_path)\n",
    "\n",
    "\n",
    "            time.sleep(3)\n",
    "            output_scientific = run_for_scientific(data[\"scientific_substantiation\"])\n",
    "            time.sleep(3)\n",
    "            output_conclusion = run_for_conclusion(data[\"conclusions\"])\n",
    "\n",
    "            try:\n",
    "                dictionary_scientific = json.loads(output_scientific)\n",
    "            except:\n",
    "                print(\"Error in scientific substantiation\")\n",
    "                print(output_scientific)\n",
    "            try:\n",
    "                dictionary_conclusion = json.loads(output_conclusion)\n",
    "            except:\n",
    "                print(\"Error in conclusion\")\n",
    "                print(output_conclusion)\n",
    "\n",
    "            \n",
    "    \n",
    "            for key in dictionary_scientific.keys():\n",
    "                scientific_text = dictionary_scientific[key]\n",
    "                if len(dictionary_conclusion) == 1:\n",
    "                    conclusion_text = dictionary_conclusion\n",
    "                else:\n",
    "                    conclusion_text = dictionary_conclusion.get(key, \"No conclusion data available\")\n",
    "                conditions_text = data['conditions_restrictions']\n",
    "                references_text = data['references']\n",
    "                save_output_to_file(folder_path, key, scientific_text, conditions_text, conclusion_text, references_text)\n",
    "\n",
    "\n",
    "def main():\n",
    "    root_directory ='RootDirectoryOfPatentData'\n",
    "    for subdir in next(os.walk(root_directory))[1]:\n",
    "        # print(subdir)\n",
    "        process_folder(os.path.join(root_directory, subdir))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
